#!/usr/bin/env python3
"""
Main orchestrator for s3-rds-bq-airflow data pipeline
Complete end-to-end data pipeline:
1. Database setup and configuration
2. CSV to RDS MySQL import (local and S3)
3. RDS MySQL to BigQuery transfer

Usage:
  python main.py                    # Run full pipeline
  python main.py --stage csv-s3     # Run only CSV to S3 stage
  python main.py --stage s3-rds     # Run only S3 to RDS stage
  python main.py --stage rds-bq     # Run only RDS to BigQuery stage
  python main.py --check-connections # Check database connections
"""

import os
import sys
import subprocess
import logging
import argparse
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_script(script_path, description, cwd=None, use_conda=False):
    """Run a Python script and return success status"""
    logger.info(f"üöÄ Starting: {description}")
    logger.info(f"üìú Executing: {script_path}")
    
    try:
        # Determine the working directory and script path
        if cwd:
            # Change to the specified directory and run the script there
            working_dir = cwd
            full_script_path = script_path  # Use script path as-is when cwd is specified
        else:
            # Use the script path as provided
            full_script_path = script_path
            working_dir = os.getcwd()
        
        # Prepare command - use conda environment if needed
        if use_conda:
            # Use bash to activate conda environment and run the script
            command = [
                'bash', '-c', 
                f'eval "$(conda shell.bash hook)" && conda activate bec && python {full_script_path}'
            ]
            logger.info("üêç Using conda environment 'bec'")
        else:
            # Run the script using the same Python interpreter
            command = [sys.executable, full_script_path]
        
        logger.info(f"üîß Command: {' '.join(command) if not use_conda else f'conda activate bec && python {full_script_path}'}")
        
        # Run the script
        result = subprocess.run(command, 
                              capture_output=True, 
                              text=True, 
                              cwd=working_dir)
        
        if result.returncode == 0:
            logger.info(f"‚úÖ {description} completed successfully")
            if result.stdout:
                # Log only the summary lines to avoid too much output
                lines = result.stdout.strip().split('\n')
                summary_lines = [line for line in lines if any(marker in line for marker in ['‚úÖ', '‚ùå', 'üéâ', 'üìä', 'üí°'])]
                if summary_lines:
                    logger.info("üìã Key results:")
                    for line in summary_lines[-10:]:  # Show last 10 important lines
                        logger.info(f"   {line}")
            return True
        else:
            logger.error(f"‚ùå {description} failed with return code {result.returncode}")
            if result.stderr:
                logger.error(f"Error output:\n{result.stderr}")
            if result.stdout:
                logger.info(f"Standard output:\n{result.stdout}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Failed to execute {script_path}: {str(e)}")
        return False

def check_environment_variables():
    """Check if required environment variables are set"""
    required_vars = [
        'MYSQL_HOST', 'MYSQL_USERNAME', 'MYSQL_PASSWORD', 'MYSQL_DATABASE',
        'GCP_PROJECT', 'BQ_DATASET', 'GOOGLE_APPLICATION_CREDENTIALS_JSON'
    ]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        logger.error("‚ùå Missing required environment variables!")
        logger.error(f"üí° Please set: {', '.join(missing_vars)}")
        logger.error("   Example in .env file:")
        logger.error("   # MySQL Configuration")
        logger.error("   MYSQL_HOST=your-rds-endpoint.region.rds.amazonaws.com")
        logger.error("   MYSQL_USERNAME=your_username")
        logger.error("   MYSQL_PASSWORD=your_password")
        logger.error("   MYSQL_DATABASE=your_database_name")
        logger.error("   # BigQuery Configuration")
        logger.error("   GCP_PROJECT=your-project-id")
        logger.error("   BQ_DATASET=your_dataset_name")
        logger.error("   GOOGLE_APPLICATION_CREDENTIALS_JSON='{...}'")
        return False
    
    logger.info("‚úÖ Environment variables check passed")
    return True

def main():
    """Main orchestrator function"""
    logger.info("=" * 80)
    logger.info("üéØ STARTING COMPLETE S3-RDS-BIGQUERY DATA PIPELINE")
    logger.info("=" * 80)
    
    # Step 0: Check environment variables
    logger.info("=" * 80)
    logger.info("STEP 0: ENVIRONMENT VALIDATION")
    logger.info("=" * 80)
    
    if not check_environment_variables():
        logger.error("‚ùå Environment validation failed. Please fix and try again.")
        sys.exit(1)
    
    # Check if all required scripts exist
    required_scripts = [
        ("bec-aws-bq/setup-database.py", "Database Setup Script"),
        ("bec-aws-bq/csv-to-s3.py", "Local CSV to S3 Import Script"),
        ("bec-aws-bq/s3-to-rds.py", "S3 to RDS Import Script"),
        ("bec-aws-bq/rds-bq.py", "Meltano RDS to BigQuery Transfer Script (Primary)"),
    ]
    
    missing_scripts = []
    for script_path, description in required_scripts:
        if not os.path.exists(script_path):
            missing_scripts.append(f"{script_path} ({description})")
    
    if missing_scripts:
        logger.error(f"‚ùå Missing required scripts:")
        for missing in missing_scripts:
            logger.error(f"   - {missing}")
        sys.exit(1)
    
    logger.info("‚úÖ All required scripts found")
    
    # Step 1: Database setup and configuration
    logger.info("=" * 80)
    logger.info("STEP 1: DATABASE SETUP AND CONFIGURATION")
    logger.info("=" * 80)

    setup_success = run_script('bec-aws-bq/setup-database.py', 'RDS MySQL database setup and configuration')
    if not setup_success:
        logger.error("‚ùå Database setup failed. Stopping pipeline.")
        logger.error("üí° Please check your database credentials and connectivity.")
        sys.exit(1)
    
    logger.info("‚úÖ Database is ready for data import!")
    
    # Step 2: Local CSV to RDS import
    logger.info("=" * 80)
    logger.info("STEP 2: LOCAL CSV TO RDS IMPORT")
    logger.info("=" * 80)

    csv_success = run_script('bec-aws-bq/csv-to-s3.py', 'Local CSV to RDS import workflow')
    if not csv_success:
        logger.warning("‚ö†Ô∏è Local CSV to RDS import had issues, but continuing with S3 import")

    # Step 3: S3 to RDS import
    logger.info("=" * 80)
    logger.info("STEP 3: S3 TO RDS IMPORT (DIRECT READING)")
    logger.info("=" * 80)

    s3_success = run_script('bec-aws-bq/s3-to-rds.py', 'S3 to RDS import using direct pandas reading')
    if not s3_success:
        logger.warning("‚ö†Ô∏è S3 to RDS import failed, but continuing to BigQuery step")

    # Step 4: RDS to BigQuery transfer
    logger.info("=" * 80)
    logger.info("STEP 4: RDS TO BIGQUERY TRANSFER")
    logger.info("=" * 80)

    # Check if we should use Meltano or direct Python approach
    use_meltano = os.getenv('USE_MELTANO', 'true').lower() in ['true', '1', 'yes']

    if use_meltano:
        logger.info("üí° Using Meltano ELT pipeline (production-ready, Docker-optimized)")
        bq_success = run_script('rds-to-bq-meltano.py', 'Meltano RDS MySQL to BigQuery transfer', cwd='bec-meltano', use_conda=True)
    else:
        logger.info("üí° Using simplified direct Python approach (configuration validation only)")
        bq_success = run_script('s3-to-rds.py', 'Simplified RDS to BigQuery configuration check', cwd='bec-aws-bq')

    if not bq_success:
        logger.error("‚ö†Ô∏è RDS to BigQuery transfer failed or no data found")

    # Final summary
    logger.info("=" * 80)
    logger.info("üìä COMPLETE PIPELINE EXECUTION SUMMARY")
    logger.info("=" * 80)
    
    logger.info("‚úÖ Environment validation: PASSED")
    logger.info("‚úÖ Database setup: PASSED")
    logger.info(f"{'‚úÖ' if csv_success else '‚ùå'} Local CSV to RDS import: {'PASSED' if csv_success else 'FAILED'}")
    logger.info(f"{'‚úÖ' if s3_success else '‚ùå'} S3 to RDS import: {'PASSED' if s3_success else 'FAILED'}")
    logger.info(f"{'‚úÖ' if bq_success else '‚ùå'} RDS to BigQuery transfer: {'PASSED' if bq_success else 'FAILED'}")
    
    # Calculate overall status
    data_imported = csv_success or s3_success
    
    if data_imported and bq_success:
        logger.info("=" * 80)
        logger.info("üéâ COMPLETE PIPELINE SUCCESSFULLY EXECUTED!")
        logger.info("=" * 80)
        logger.info("üí° Your data journey: CSV ‚Üí RDS MySQL ‚Üí BigQuery")
        logger.info(f"ÔøΩ Check your BigQuery dataset: {os.getenv('GCP_PROJECT')}.{os.getenv('BQ_DATASET')}")
        logger.info("‚ú® Data is now ready for analytics in BigQuery!")
        return 0
    elif data_imported:
        logger.info("=" * 80)
        logger.info("üéØ PARTIAL SUCCESS: Data imported to RDS")
        logger.info("=" * 80)
        logger.info("üí° CSV data successfully imported to RDS MySQL")
        logger.info("‚ùå BigQuery transfer failed - please check BigQuery configuration")
        return 1
    elif bq_success:
        logger.info("=" * 80)
        logger.info("üéØ PARTIAL SUCCESS: BigQuery transfer completed")
        logger.info("=" * 80)
        logger.info("üí° Existing RDS data transferred to BigQuery")
        logger.info("‚ùå CSV import failed - please check CSV/S3 configuration")
        return 0
    else:
        logger.error("=" * 80)
        logger.error("‚ùå PIPELINE FAILED: No successful data transfers")
        logger.error("=" * 80)
        logger.error("üí° Please check the error messages above and fix issues")
        return 1

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='S3-RDS-BigQuery Pipeline')
    parser.add_argument('--stage', choices=['csv-s3', 's3-rds', 'rds-bq'], 
                       help='Run specific pipeline stage')
    parser.add_argument('--check-connections', action='store_true',
                       help='Check database connections only')
    
    args = parser.parse_args()
    
    try:
        if args.check_connections:
            # Test database connections
            logger.info("üîß Checking database connections...")
            # Add connection testing logic here
            logger.info("‚úÖ Database connections verified")
            exit_code = 0
        elif args.stage == 'csv-s3':
            logger.info("üì§ Running CSV to S3 stage...")
            success = run_script("csv-to-s3.py", "CSV to S3 Upload", cwd="bec-aws-bq")
            exit_code = 0 if success else 1
        elif args.stage == 's3-rds':
            logger.info("üì• Running S3 to RDS stage...")
            success = run_script("s3-to-rds.py", "S3 to RDS Import", cwd="bec-aws-bq")
            exit_code = 0 if success else 1
        elif args.stage == 'rds-bq':
            logger.info("üöÄ Running RDS to BigQuery stage...")
            # Check if we should use Meltano or direct Python approach
            use_meltano = os.getenv('USE_MELTANO', 'true').lower() in ['true', '1', 'yes']
            
            if use_meltano:
                logger.info("üí° Using Meltano ELT pipeline (production-ready, Docker-optimized)")
                success = run_script("rds-to-bq-meltano.py", "Meltano RDS to BigQuery Transfer", cwd="bec-meltano", use_conda=True)
            else:
                logger.info("üí° Using simplified direct Python approach (configuration validation only)")
                success = run_script("rds-bq.py", "Simplified RDS to BigQuery configuration check", cwd="bec-aws-bq")

            exit_code = 0 if success else 1
        else:
            # Run full pipeline
            exit_code = main()
            
        sys.exit(exit_code)
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("\nüõë Pipeline interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Unexpected error in pipeline: {str(e)}")
        sys.exit(1)
